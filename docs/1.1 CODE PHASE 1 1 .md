# CODE PHASE 1.1

# [Photostudio.io](http://photostudio.io/) Garment Preprocessing Pipeline

# Phase 1: Advanced Image Segmentation & JSON Generation

# MIT-grade Computer Vision Implementation — PRODUCTION READY (v1.1.0)

# 

# Dependencies (Python 3.10+):

# pip install opencv-python numpy pillow scikit-image scikit-learn google-generativeai jsonschema

# 

# Notes:

# Gemini analysis is optional; pass --no-gemini to skip.

# Deterministic where practical: fixed KMeans random_state; deterministic pixel sampling; seeded RNG.

# Outputs per input: alpha.png (RGBA), label_crop.png (if found), mask.png, garment_analysis.json, quality_report.json

from **future** import annotations

import os
import cv2
import json
import uuid
import math
import time
import argparse
import logging
import tempfile
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

import numpy as np
from PIL import Image, ImageOps
from sklearn.cluster import KMeans

# Optional dependency (loaded lazily)

try:
import google.generativeai as genai  # type: ignore
except Exception:
genai = None  # type: ignore

# ---------------------------------------------------------------------------

# Logging

# ---------------------------------------------------------------------------

logging.basicConfig(
[level=logging.INFO](http://level=logging.info/),
format="%(asctime)s.%(msecs)03d %(levelname)s [%(name)s] %(message)s",
datefmt="%H:%M:%S",
)
logger = logging.getLogger("photostudio.preprocess")

# Global reproducibility

np.random.seed(42)

# ---------------------------------------------------------------------------

# Data Structures

# ---------------------------------------------------------------------------

class GarmentCategory(Enum):
TOPS = "tops"
BOTTOMS = "bottoms"
DRESSES = "dresses"
OUTERWEAR = "outerwear"
ACCESSORIES = "accessories"
UNKNOWN = "unknown"

@dataclass
class QualityMetrics:
resolution_score: float
lighting_score: float
focus_score: float
contrast_score: float
overall_score: float
issues: List[str]

@dataclass
class SegmentationResult:
garment_mask: np.ndarray
confidence_score: float
boundary_irregularity: float  # 1 - circularity
background_complexity: str
edge_quality: float
contour_points: List[Tuple[int, int]]
fragmentation: int
bbox_norm: Optional[List[float]]  # [x0,y0,x1,y1] in [0,1]

# ---------------------------------------------------------------------------

# JSON Schema (lightweight; enforced when jsonschema is available)

# ---------------------------------------------------------------------------

SCHEMA: Dict[str, Any] = {
"type": "object",
"required": [
"processing_metadata",
"input_validation",
"segmentation_results",
"fabric_analysis",
"color_extraction",
"garment_categorization",
"construction_details",
"brand_information",
"distinctive_features",
"quality_validation",
"schema_version",
"schema_compliance",
],
"properties": {
"processing_metadata": {"type": "object"},
"input_validation": {"type": "object"},
"segmentation_results": {"type": "object"},
"fabric_analysis": {"type": "object"},
"color_extraction": {"type": "object"},
"garment_categorization": {"type": "object"},
"construction_details": {"type": "object"},
"brand_information": {"type": "object"},
"distinctive_features": {"type": "object"},
"quality_validation": {"type": "object"},
"schema_version": {"type": "string"},
"schema_compliance": {"type": "boolean"},
},
}

# ---------------------------------------------------------------------------

# Preprocessor

# ---------------------------------------------------------------------------

class AdvancedGarmentPreprocessor:
"""
Garment preprocessing system for ghost mannequin generation (Step 1).
Stages:
- Load + EXIF fix + gray-world WB
- Segmentation (GrabCut or refine precomputed)
- Deskew/center/pad → square
- Fabric/material analysis (LBP)
- Color extraction (LAB + KMeans)
- Optional Gemini 2.5 Pro JSON analysis (strict JSON)
- Construction heuristics (closures/pockets)
- Compile JSON
- Export alpha RGBA + label crop
"""

```
def __init__(
    self,
    gemini_api_key: Optional[str] = None,
    model_name: str = "gemini-2.5-pro",
    min_resolution: int = 768,
    min_contrast: float = 0.28,
    min_focus: float = 0.45,
    min_lighting: float = 0.40,
    target_size: int = 1536,
    enable_gemini: bool = True,
):
    self.target_size = int(target_size)
    self.enable_gemini = bool(enable_gemini)

    self.quality_thresholds = {
        "min_resolution": int(min_resolution),
        "min_contrast": float(min_contrast),
        "min_focus": float(min_focus),
        "min_lighting": float(min_lighting),
    }

    self.gemini_model_name = model_name
    self.gemini_ready = False
    if self.enable_gemini:
        if genai is None:
            logger.warning("google-generativeai not installed; disabling Gemini stage.")
            self.enable_gemini = False
        else:
            if not gemini_api_key:
                gemini_api_key = os.getenv("GEMINI_API_KEY", "")
            if not gemini_api_key:
                logger.warning("No GEMINI_API_KEY found; disabling Gemini stage.")
                self.enable_gemini = False
            else:
                try:
                    genai.configure(api_key=gemini_api_key)  # type: ignore
                    self.gemini_model = genai.GenerativeModel(self.gemini_model_name)  # type: ignore
                    self.gemini_ready = True
                except Exception as e:
                    logger.warning(f"Gemini init failed: {e}. Disabling Gemini stage.")
                    self.enable_gemini = False
                    self.gemini_ready = False

# ---------------------------
# Public API
# ---------------------------
def process_garment(
    self,
    image_path: str,
    precomputed_mask: Optional[np.ndarray] = None,
    out_dir: Optional[Path] = None,
) -> Dict[str, Any]:
    """
    Run preprocessing pipeline on a single image.
    Returns: comprehensive JSON dict.
    """
    if out_dir is None:
        out_dir = Path("output") / Path(image_path).stem
    out_dir.mkdir(parents=True, exist_ok=True)

    logger.info(f"Processing garment: {image_path}")

    # Stage 0: Load + EXIF fix (BGR in OpenCV)
    bgr = cv2.imread(image_path, cv2.IMREAD_COLOR)
    if bgr is None:
        raise ValueError(f"Cannot load image: {image_path}")
    bgr = self._fix_exif(bgr)

    # Convert to RGB (work in RGB space)
    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)

    # Stage 1: Quality Assessment (pre-WB, for diagnostics)
    quality_initial = self._assess_image_quality(rgb)

    # Stage 2: Gray-world white balance to neutralize casts
    rgb = self._wb_gray_world(rgb)

    # Stage 3: Segmentation (precomputed or GrabCut) + refinement
    seg = self._segment_garment(rgb, precomputed_mask=precomputed_mask)

    # Stage 4: Alignment (deskew/center/pad) using mask PCA
    rgb_aligned, mask_aligned = self._deskew_center_pad(rgb, seg.garment_mask, target=self.target_size)

    # Recompute segmentation metrics on aligned mask (stable for downstream)
    seg_aligned = self._recompute_seg_metrics(rgb_aligned, mask_aligned)

    # Save mask preview
    cv2.imwrite(str(out_dir / "mask.png"), mask_aligned)

    # Stage 5: Fabric & Material Analysis
    fabric_analysis = self._analyze_fabric(rgb_aligned, mask_aligned)

    # Stage 6: Color Extraction (deterministic sampling)
    color_data = self._extract_colors(rgb_aligned, mask_aligned)

    # Stage 7: Optional Gemini JSON Analysis (use processed image for stability)
    gemini_analysis = self._default_gemini_response()
    if self.enable_gemini and self.gemini_ready:
        gemini_analysis = self._analyze_with_gemini(rgb_aligned)

    # Stage 8: Construction Details (heuristics)
    construction = self._analyze_construction(rgb_aligned, mask_aligned)

    # Stage 9: Export alpha RGBA + label crop
    label_bbox_norm = self._export_alpha_and_label(rgb_aligned, mask_aligned, out_dir)

    # Stage 10: Compile JSON
    json_out = self._compile_json_output(
        quality_initial,
        seg_aligned,
        fabric_analysis,
        color_data,
        gemini_analysis,
        construction,
        label_bbox_norm,
    )

    # Stage 11: Strict schema validation (if jsonschema available)
    self._validate_json_hard(json_out)

    # Save JSON
    with open(out_dir / "garment_analysis.json", "w", encoding="utf-8") as f:
        json.dump(json_out, f, indent=2)

    # Quality report
    quality_report = QualityValidator().generate_quality_report(json_out)
    with open(out_dir / "quality_report.json", "w", encoding="utf-8") as f:
        json.dump(quality_report, f, indent=2)

    logger.info(f"Done: {image_path} → {out_dir}")
    return json_out

# ---------------------------
# Stage implementations
# ---------------------------
def _fix_exif(self, bgr: np.ndarray) -> np.ndarray:
    """Honor EXIF orientation using PIL, then return BGR ndarray."""
    try:
        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
        pil = Image.fromarray(rgb)
        pil = ImageOps.exif_transpose(pil)
        out = cv2.cvtColor(np.array(pil), cv2.COLOR_RGB2BGR)
        return out
    except Exception:
        return bgr

def _wb_gray_world(self, rgb: np.ndarray) -> np.ndarray:
    """Simple gray-world white balance."""
    gains = np.mean(rgb, axis=(0, 1)).astype(np.float32)
    gains /= np.mean(gains)
    out = np.clip(rgb.astype(np.float32) / gains, 0, 255).astype(np.uint8)
    return out

def _assess_image_quality(self, image_rgb: np.ndarray) -> QualityMetrics:
    """Assess resolution, lighting, focus, contrast; produce issues list."""
    h, w = image_rgb.shape[:2]
    issues: List[str] = []

    # Resolution
    resolution_score = min(1.0, min(h, w) / float(self.quality_thresholds["min_resolution"]))
    if resolution_score < 1.0:
        issues.append(f"Low resolution: {min(h, w)}px")

    gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)
    mean_brightness = float(np.mean(gray)) / 255.0
    std_brightness = float(np.std(gray)) / 255.0
    lighting_score = max(0.0, min(1.0, 1.0 - abs(mean_brightness - 0.5) * 2.0))
    if mean_brightness < 0.3:
        issues.append("Underexposed")
    elif mean_brightness > 0.7:
        issues.append("Overexposed")
    if lighting_score < self.quality_thresholds["min_lighting"]:
        issues.append("Lighting below threshold")

    laplacian = cv2.Laplacian(gray, cv2.CV_64F)
    focus_score = max(0.0, min(1.0, float(np.var(laplacian)) / 1000.0))
    if focus_score < self.quality_thresholds["min_focus"]:
        issues.append("Poor focus/blur detected")

    contrast_score = max(0.0, min(1.0, std_brightness * 2.0))
    if contrast_score < self.quality_thresholds["min_contrast"]:
        issues.append("Low contrast")

    overall_score = (resolution_score + lighting_score + focus_score + contrast_score) / 4.0

    return QualityMetrics(
        resolution_score=round(resolution_score, 3),
        lighting_score=round(lighting_score, 3),
        focus_score=round(focus_score, 3),
        contrast_score=round(contrast_score, 3),
        overall_score=round(overall_score, 3),
        issues=issues,
    )

def _segment_garment(
    self,
    image_rgb: np.ndarray,
    precomputed_mask: Optional[np.ndarray] = None,
) -> SegmentationResult:
    """Segment garment; refine via largest-contour + hole-fill + GrabCut-with-mask."""
    h, w = image_rgb.shape[:2]
    if precomputed_mask is not None:
        logger.info("Using precomputed mask.")
        base_mask = (precomputed_mask > 0).astype(np.uint8) * 255
    else:
        # GrabCut with centered rect init
        logger.info("Running GrabCut init…")
        mask = np.zeros((h, w), np.uint8)
        bgd_model = np.zeros((1, 65), np.float64)
        fgd_model = np.zeros((1, 65), np.float64)
        rect = (int(w * 0.08), int(h * 0.08), int(w * 0.84), int(h * 0.84))
        try:
            cv2.grabCut(image_rgb, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)
        except Exception as e:
            logger.warning(f"GrabCut failed ({e}); retrying fewer iterations.")
            try:
                cv2.grabCut(image_rgb, mask, rect, bgd_model, fgd_model, 3, cv2.GC_INIT_WITH_RECT)
            except Exception as e2:
                logger.error(f"GrabCut fallback failed ({e2}); returning empty mask.")
                empty = np.zeros((h, w), np.uint8)
                return self._recompute_seg_metrics(image_rgb, empty)
        base_mask = np.where((mask == 2) | (mask == 0), 0, 255).astype("uint8")

    # Refinement: keep largest component, fill holes, then GrabCut with mask
    mask_largest = self._fill_holes_largest(base_mask)
    final_mask = self._refine_grabcut_with_mask(image_rgb, mask_largest)

    return self._recompute_seg_metrics(image_rgb, final_mask)

def _fill_holes_largest(self, mask: np.ndarray) -> np.ndarray:
    if mask is None or mask.max() == 0:
        return np.zeros_like(mask)
    cnts, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not cnts:
        return np.zeros_like(mask)
    c = max(cnts, key=cv2.contourArea)
    filled = np.zeros_like(mask)
    cv2.drawContours(filled, [c], -1, 255, -1)
    # Fill internal holes via flood-fill on inverse
    inv = 255 - filled
    h, w = inv.shape
    flood = inv.copy()
    cv2.floodFill(flood, np.zeros((h + 2, w + 2), np.uint8), (0, 0), 0)
    holes = inv - flood
    return filled + holes

def _refine_grabcut_with_mask(self, rgb: np.ndarray, mask: np.ndarray) -> np.ndarray:
    if mask is None or mask.max() == 0:
        return np.zeros_like(mask)
    gc_mask = np.where(mask > 0, cv2.GC_PR_FGD, cv2.GC_PR_BGD).astype("uint8")
    bgd = np.zeros((1, 65), np.float64)
    fgd = np.zeros((1, 65), np.float64)
    try:
        cv2.grabCut(rgb, gc_mask, None, bgd, fgd, 3, cv2.GC_INIT_WITH_MASK)
        out = np.where((gc_mask == cv2.GC_FGD) | (gc_mask == cv2.GC_PR_FGD), 255, 0).astype("uint8")
    except Exception as e:
        logger.warning(f"GrabCut refine failed: {e}; using input mask.")
        out = (mask > 0).astype(np.uint8) * 255
    return out

def _recompute_seg_metrics(self, image_rgb: np.ndarray, mask: np.ndarray) -> SegmentationResult:
    gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if contours:
        largest = max(contours, key=cv2.contourArea)
        area = float(cv2.contourArea(largest))
        perimeter = float(cv2.arcLength(largest, True))
        circularity = 4.0 * np.pi * area / (perimeter * perimeter) if perimeter > 0 else 0.0
        circularity = float(np.clip(circularity, 0.0, 1.0))
        boundary_irregularity = 1.0 - circularity

        # Edge quality within 3px band around boundary
        edges = cv2.Canny(gray, 50, 150)
        dist = cv2.distanceTransform(255 - mask, cv2.DIST_L2, 3)
        band = ((dist > 0) & (dist <= 3)).astype(np.uint8)
        boundary_edges = float(np.sum((edges > 0) & (band > 0)))
        edge_quality = min(1.0, boundary_edges / max(1.0, perimeter))

        # Background complexity via std of bg gray
        bg_mask = cv2.bitwise_not(mask)
        bg_gray = cv2.bitwise_and(gray, gray, mask=bg_mask)
        vals = bg_gray[bg_gray > 0]
        bg_std = float(np.std(vals)) if vals.size else 0.0
        if bg_std < 20:
            bg_complexity = "clean"
        elif bg_std < 50:
            bg_complexity = "partial"
        else:
            bg_complexity = "complex"

        epsilon = 0.01 * perimeter
        approx = cv2.approxPolyDP(largest, epsilon, True)
        contour_points = [(int(p[0][0]), int(p[0][1])) for p in approx[:200]]

        # Fragmentation (components count)
        ncomp, _ = cv2.connectedComponents((mask > 0).astype(np.uint8))
        fragmentation = int(max(0, ncomp - 1))

        confidence = float((circularity + edge_quality) / 2.0)

        # Normalized bbox
        ys, xs = np.where(mask > 0)
        x0, y0, x1, y1 = xs.min(), ys.min(), xs.max(), ys.max()
        h, w = mask.shape
        bbox_norm = [x0 / w, y0 / h, x1 / w, y1 / h]

    else:
        boundary_irregularity = 1.0
        edge_quality = 0.0
        bg_complexity = "unknown"
        contour_points = []
        confidence = 0.0
        fragmentation = 0
        bbox_norm = None

    return SegmentationResult(
        garment_mask=mask,
        confidence_score=round(confidence, 3),
        boundary_irregularity=round(boundary_irregularity, 3),
        background_complexity=bg_complexity,
        edge_quality=round(edge_quality, 3),
        contour_points=contour_points,
        fragmentation=fragmentation,
        bbox_norm=bbox_norm,
    )

def _deskew_center_pad(self, rgb: np.ndarray, mask: np.ndarray, target: int) -> Tuple[np.ndarray, np.ndarray]:
    """Rotate by principal axis; center bbox; pad square; resize to target."""
    ys, xs = np.where(mask > 0)
    if len(xs) == 0:
        # Nothing to align
        s = max(rgb.shape[:2])
        sq = np.full((s, s, 3), 255, np.uint8)
        msk = np.zeros((s, s), np.uint8)
        out_rgb = cv2.resize(sq, (target, target), interpolation=cv2.INTER_AREA)
        out_msk = cv2.resize(msk, (target, target), interpolation=cv2.INTER_NEAREST)
        return out_rgb, out_msk

    pts = np.column_stack([xs, ys]).astype(np.float32)
    pts_centered = pts - pts.mean(0)
    _, _, vt = np.linalg.svd(pts_centered, full_matrices=False)
    angle = math.degrees(math.atan2(vt[0, 1], vt[0, 0]))

    (h, w) = rgb.shape[:2]
    M = cv2.getRotationMatrix2D((w / 2, h / 2), angle, 1.0)
    rgbR = cv2.warpAffine(rgb, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)
    maskR = cv2.warpAffine(mask, M, (w, h), flags=cv2.INTER_NEAREST, borderMode=cv2.BORDER_CONSTANT)

    ys, xs = np.where(maskR > 0)
    x0, y0, x1, y1 = xs.min(), ys.min(), xs.max(), ys.max()
    cx, cy = (x0 + x1) // 2, (y0 + y1) // 2
    side = int(1.2 * max(x1 - x0, y1 - y0))
    half = side // 2
    xA, yA = max(0, cx - half), max(0, cy - half)
    xB, yB = min(w, cx + half), min(h, cy + half)

    crop_rgb = rgbR[yA:yB, xA:xB]
    crop_msk = maskR[yA:yB, xA:xB]

    s = max(crop_rgb.shape[:2])
    pad_rgb = np.full((s, s, 3), 255, np.uint8)
    pad_msk = np.zeros((s, s), np.uint8)
    oy = (s - crop_rgb.shape[0]) // 2
    ox = (s - crop_rgb.shape[1]) // 2
    pad_rgb[oy : oy + crop_rgb.shape[0], ox : ox + crop_rgb.shape[1]] = crop_rgb
    pad_msk[oy : oy + crop_msk.shape[0], ox : ox + crop_msk.shape[1]] = crop_msk

    out_rgb = cv2.resize(pad_rgb, (target, target), interpolation=cv2.INTER_AREA)
    out_msk = cv2.resize(pad_msk, (target, target), interpolation=cv2.INTER_NEAREST)
    return out_rgb, out_msk

def _analyze_fabric(self, image_rgb: np.ndarray, mask: np.ndarray) -> Dict[str, Any]:
    """Texture via LBP + simple reflectance heuristics."""
    try:
        from skimage import feature  # local import
    except Exception:
        logger.warning("scikit-image missing; fabric analysis limited.")
        return {
            "texture_classification": {
                "primary_texture": "unknown",
                "surface_properties": [],
                "fabric_weight": "unknown",
                "texture_variance": 0.0,
            },
            "material_detection": {
                "material_type": "unknown",
                "drape_behavior": "unknown",
                "stretch_potential": "unknown",
                "reflectance_metrics": {"mean_saturation": 0.0, "mean_value": 0.0, "value_std": 0.0},
            },
        }

    garment_rgb = cv2.bitwise_and(image_rgb, image_rgb, mask=mask)
    gray = cv2.cvtColor(garment_rgb, cv2.COLOR_RGB2GRAY)

    # LBP texture
    radius = 3
    n_points = 8 * radius
    lbp = feature.local_binary_pattern(gray, n_points, radius, method="uniform")
    lbp_hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))
    lbp_hist = lbp_hist.astype("float")
    lbp_hist /= (lbp_hist.sum() + 1e-6)
    texture_variance = float(np.var(lbp_hist))

    if texture_variance < 0.001:
        primary_texture = "smooth"
        surface_properties = ["smooth", "uniform"]
    elif texture_variance < 0.005:
        primary_texture = "fine_weave"
        surface_properties = ["woven", "fine"]
    elif texture_variance < 0.01:
        primary_texture = "textured"
        surface_properties = ["textured", "medium"]
    else:
        primary_texture = "coarse"
        surface_properties = ["ribbed", "coarse"]

    hsv = cv2.cvtColor(garment_rgb, cv2.COLOR_RGB2HSV)
    sat = hsv[:, :, 1][mask > 0]
    val = hsv[:, :, 2][mask > 0]

    mean_sat = float(np.mean(sat)) if sat.size else 0.0
    mean_val = float(np.mean(val)) if val.size else 0.0
    std_val = float(np.std(val)) if val.size else 0.0

    if std_val > 50:
        material_type = "synthetic"
        drape_behavior = "structured"
    elif mean_sat < 30 and mean_val > 200:
        material_type = "cotton"
        drape_behavior = "moderate"
    elif mean_sat > 100:
        material_type = "polyester_blend"
        drape_behavior = "flowing"
    else:
        material_type = "natural_fiber"
        drape_behavior = "soft"

    edges = cv2.Canny(gray, 50, 150)
    denom = float(np.sum(mask > 0))
    edge_density = float(np.sum(edges > 0)) / denom if denom > 0 else 0.0
    if edge_density < 0.05:
        fabric_weight = "light"
        stretch_potential = "high"
    elif edge_density < 0.1:
        fabric_weight = "medium"
        stretch_potential = "moderate"
    else:
        fabric_weight = "heavy"
        stretch_potential = "low"

    return {
        "texture_classification": {
            "primary_texture": primary_texture,
            "surface_properties": surface_properties,
            "fabric_weight": fabric_weight,
            "texture_variance": round(texture_variance, 4),
        },
        "material_detection": {
            "material_type": material_type,
            "drape_behavior": drape_behavior,
            "stretch_potential": stretch_potential,
            "reflectance_metrics": {
                "mean_saturation": round(mean_sat, 2),
                "mean_value": round(mean_val, 2),
                "value_std": round(std_val, 2),
            },
        },
    }

def _extract_colors(self, image_rgb: np.ndarray, mask: np.ndarray) -> Dict[str, Any]:
    """
    LAB-based color extraction with deterministic sampling (stride).
    Produces primary + secondary colors and simple pattern meta.
    """
    garment = image_rgb[mask > 0]
    if garment.size == 0:
        return self._empty_color_data()

    region = cv2.bitwise_and(image_rgb, image_rgb, mask=mask)
    lab = cv2.cvtColor(region, cv2.COLOR_RGB2LAB)
    lab_pixels = lab[mask > 0]

    # Deterministic sampling to ~<=25k pixels
    n = len(lab_pixels)
    if n > 25000:
        stride = int(math.ceil(n / 25000))
        sample = lab_pixels[::stride]
    else:
        sample = lab_pixels

    # KMeans clusters
    n_colors = int(min(5, max(1, len(sample) // 5000))) or 1
    kmeans = KMeans(n_clusters=n_colors, random_state=42, n_init=10)
    kmeans.fit(sample)
    labels = kmeans.predict(sample)
    centers = kmeans.cluster_centers_
    counts = np.bincount(labels, minlength=n_colors)
    percentages = counts / float(len(labels))
    order = np.argsort(percentages)[::-1]

    def lab_to_hex(lab_color: np.ndarray) -> Tuple[str, List[int]]:
        rgb = cv2.cvtColor(np.uint8([[lab_color]]), cv2.COLOR_LAB2RGB)[0][0]
        return "#{:02x}{:02x}{:02x}".format(int(rgb[0]), int(rgb[1]), int(rgb[2])), rgb.tolist()

    primary_lab = centers[order[0]]
    primary_hex, primary_rgb = lab_to_hex(primary_lab)

    colors = []
    for idx in order[:3]:
        hx, rgb = lab_to_hex(centers[idx])
        colors.append(
            {
                "hex_value": hx,
                "lab_values": centers[idx].tolist(),
                "rgb_values": rgb,
                "percentage": round(float(percentages[idx]) * 100.0, 2),
            }
        )

    pattern_meta = self._detect_pattern(image_rgb, mask)

    return {
        "primary_color": {
            "hex_value": primary_hex,
            "lab_values": primary_lab.tolist(),
            "rgb_values": primary_rgb,
            "color_name": self._get_color_name(primary_rgb),
        },
        "secondary_colors": colors[1:] if len(colors) > 1 else [],
        "pattern_analysis": pattern_meta,
        "color_distribution": {
            "dominant_percentage": round(float(percentages[order[0]]) * 100.0, 2),
            "color_variety": int(n_colors),
        },
    }

def _detect_pattern(self, image_rgb: np.ndarray, mask: np.ndarray) -> Dict[str, Any]:
    gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)
    garment_gray = cv2.bitwise_and(gray, gray, mask=mask)
    if np.count_nonzero(mask) == 0:
        return {"pattern_type": "unknown", "pattern_scale": "unknown", "pattern_complexity": 0.0}

    # Frequency-domain peak density (cheap approximate)
    f_transform = np.fft.fft2(garment_gray.astype(np.float32))
    f_shift = np.fft.fftshift(f_transform)
    magnitude = np.abs(f_shift)
    threshold = np.percentile(magnitude, 99)
    peaks = magnitude > threshold
    peak_count = int(np.sum(peaks))

    if peak_count < 10:
        pattern_type = "solid"
        pattern_scale = "none"
    elif peak_count < 50:
        edges = cv2.Canny(garment_gray, 50, 150)
        lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 50, minLineLength=30, maxLineGap=10)
        if lines is not None and len(lines) > 20:
            pattern_type = "stripe"
        else:
            pattern_type = "geometric"
        pattern_scale = "medium"
    else:
        pattern_type = "complex"
        pattern_scale = "varied"

    return {
        "pattern_type": pattern_type,
        "pattern_scale": pattern_scale,
        "pattern_complexity": round(peak_count / 100.0, 2),
    }

def _analyze_construction(self, image_rgb: np.ndarray, mask: np.ndarray) -> Dict[str, Any]:
    gray = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2GRAY)
    garment_gray = cv2.bitwise_and(gray, gray, mask=mask)

    # Buttons (Hough circles) — cap false positives on textured fabrics
    circles = cv2.HoughCircles(
        garment_gray, cv2.HOUGH_GRADIENT, dp=1.2, minDist=24, param1=80, param2=30, minRadius=5, maxRadius=20
    )
    closure_count = 0
    closure_type = "unknown"
    if circles is not None and len(circles) > 0 and len(circles[0]) <= 12:
        closure_count = len(circles[0])
        closure_type = "buttons"

    # Zipper via roughly parallel lines
    edges = cv2.Canny(garment_gray, 50, 150)
    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 120, minLineLength=60, maxLineGap=6)
    has_zipper = False
    if lines is not None and len(lines) < 120:
        limit = min(len(lines), 24)
        for i in range(limit):
            for j in range(i + 1, limit):
                x1, y1, x2, y2 = lines[i][0]
                x3, y3, x4, y4 = lines[j][0]
                angle1 = math.atan2(y2 - y1, x2 - x1)
                angle2 = math.atan2(y4 - y3, x4 - x3)
                if abs(angle1 - angle2) < 0.1:
                    # distance between lines
                    dist = abs((y3 - y1) * math.cos(angle1) - (x3 - x1) * math.sin(angle1))
                    if 5 < dist < 30:
                        has_zipper = True
                        break
            if has_zipper:
                break
    if has_zipper:
        closure_type = "zipper"

    # Pockets (simple rectangular contour heuristic)
    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    pockets_detected = False
    pocket_type = "none"
    for contour in contours[:80]:
        area = cv2.contourArea(contour)
        if 600 < area < 6000:
            perimeter = cv2.arcLength(contour, True)
            approx = cv2.approxPolyDP(contour, 0.02 * perimeter, True)
            if len(approx) == 4:
                pockets_detected = True
                pocket_type = "patch"
                break

    return {
        "closures": {
            "primary_closure": closure_type,
            "closure_count": int(closure_count),
            "closure_position": "front",
            "confidence": 0.7 if closure_type != "unknown" else 0.4,
        },
        "critical_features": {
            "pockets": {"present": bool(pockets_detected), "type": pocket_type, "count": 1 if pockets_detected else 0},
            "embellishments": {"present": False, "types": []},
            "hardware": {
                "present": bool(closure_count > 0 or has_zipper),
                "material": "metal" if closure_type == "zipper" else ("plastic" if closure_type == "buttons" else "unknown"),
            },
        },
        "stitching_quality": {"visible_seams": True, "seam_type": "standard", "quality_score": 0.8},
    }

def _analyze_with_gemini(self, image_rgb: np.ndarray) -> Dict[str, Any]:
    """Send processed image to Gemini 2.5 Pro with strict JSON output."""
    if not (self.enable_gemini and self.gemini_ready):
        return self._default_gemini_response()

    prompt = """

```

Return only valid JSON. Do not wrap in markdown.
Analyze this garment image and return a JSON object with the following structure.
Use "unknown" when not sure.

{
"garment_categorization": {
"primary_classification": {
"category": "tops/bottoms/dresses/outerwear/accessories",
"specific_type": "t-shirt/shirt/pants/skirt/jacket/etc",
"subcategory": "casual/formal/athletic/etc",
"classification_confidence": 0.0
},
"style_attributes": {
"silhouette": "fitted/loose/oversized/tailored/slim",
"neckline": "crew/v-neck/scoop/boat/collar/hood",
"sleeves": "sleeveless/short/long/three-quarter",
"hemline": "straight/curved/asymmetric",
"fit_type": "regular/slim/relaxed/oversized"
}
},
"brand_and_labels": {
"brand_visible": false,
"brand_name": "unknown",
"label_text": "",
"label_position": "unknown",
"care_labels_visible": false
},
"distinctive_features": {
"unique_elements": [],
"prints_or_graphics": "none",
"embroidery": false,
"appliques": false,
"distressing": false
},
"size_estimation": {
"apparent_size": "unknown",
"length_category": "regular",
"width_category": "regular"
},
"condition_assessment": {
"overall_condition": "new",
"visible_defects": [],
"presentation_quality": 0.9
},
"technical_details": {
"seam_visibility": true,
"construction_quality": "medium",
"finishing_details": [],
"symmetric": true
}
}
""".strip()

```
    # Save processed image to a temp file for upload
    with tempfile.TemporaryDirectory() as td:
        tmp = Path(td) / "proc.jpg"
        # encode RGB to BGR for cv2.imwrite
        cv2.imwrite(str(tmp), cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR), [int(cv2.IMWRITE_JPEG_QUALITY), 95])

        try:
            image_file = genai.upload_file(str(tmp))  # type: ignore
        except Exception as e:
            logger.error(f"Gemini upload failed: {e}")
            return self._default_gemini_response()

        try:
            resp = self.gemini_model.generate_content(  # type: ignore
                [prompt, image_file],
                generation_config=genai.GenerationConfig(  # type: ignore
                    temperature=0.1,
                    top_p=0.3,
                    top_k=1,
                    max_output_tokens=2048,
                    response_mime_type="application/json",
                ),
            )
            try:
                data = resp.parsed if hasattr(resp, "parsed") else json.loads(resp.text)
            except Exception:
                logger.exception("Failed to parse Gemini JSON; using default.")
                data = self._default_gemini_response()
        except Exception as e:
            logger.error(f"Gemini analysis failed: {e}")
            data = self._default_gemini_response()
        finally:
            try:
                genai.delete_file(image_file.name)  # type: ignore
            except Exception:
                pass

    return data

# ---------------------------
# Output compilation & helpers
# ---------------------------
def _export_alpha_and_label(self, rgb: np.ndarray, mask: np.ndarray, out_dir: Path) -> Optional[List[float]]:
    """Export RGBA alpha cutout and try to crop inner-neck label area; returns label bbox in [0,1] if saved."""
    # Save alpha RGBA
    rgba = np.dstack([rgb, mask])
    cv2.imwrite(str(out_dir / "alpha.png"), cv2.cvtColor(rgba, cv2.COLOR_RGBA2BGRA))

    # Heuristic label ROI: top-central 30% height, center 50% width
    h, w = mask.shape
    x0 = int(w * 0.25)
    x1 = int(w * 0.75)
    y1 = int(h * 0.30)
    roi = mask[:y1, x0:x1]
    if roi.max() == 0:
        return None

    ys, xs = np.where(roi > 0)
    if len(xs) < 50:
        return None

    # Tight bbox in ROI
    lx0, lx1 = xs.min(), xs.max()
    ly0, ly1 = ys.min(), ys.max()

    # Expand a bit
    pad = 8
    gx0 = max(0, x0 + lx0 - pad)
    gx1 = min(w, x0 + lx1 + pad)
    gy0 = max(0, ly0 - pad)
    gy1 = min(h, ly1 + pad)

    crop = rgb[gy0:gy1, gx0:gx1]
    if crop.size == 0:
        return None
    cv2.imwrite(str(out_dir / "label_crop.png"), cv2.cvtColor(crop, cv2.COLOR_RGB2BGR))

    # Normalized bbox
    return [gx0 / w, gy0 / h, gx1 / w, gy1 / h]

def _compile_json_output(
    self,
    quality: QualityMetrics,
    seg: SegmentationResult,
    fabric: Dict[str, Any],
    colors: Dict[str, Any],
    gemini: Dict[str, Any],
    construction: Dict[str, Any],
    label_bbox_norm: Optional[List[float]],
) -> Dict[str, Any]:
    session_id = uuid.uuid4().hex[:12]
    mask_cov = float(np.sum(seg.garment_mask > 0)) / max(1, seg.garment_mask.size)

    out = {
        "processing_metadata": {
            "session_id": session_id,
            "pipeline_version": "1.1.0",
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
            "processing_stages_completed": 10,
            "target_size": self.target_size,
        },
        "input_validation": {
            "image_quality_score": quality.overall_score,
            "resolution_check": f"{quality.resolution_score:.2f}",
            "lighting_assessment": self._classify_lighting(quality.lighting_score),
            "focus_quality": self._classify_focus(quality.focus_score),
            "quality_issues": quality.issues,
            "preprocessing_suitable": quality.overall_score > 0.6,
        },
        "segmentation_results": {
            "confidence_score": seg.confidence_score,
            "boundary_irregularity": seg.boundary_irregularity,
            "boundary_smoothness": round(1.0 - seg.boundary_irregularity, 3),
            "background_separation": seg.background_complexity,
            "edge_quality": seg.edge_quality,
            "mask_coverage": round(mask_cov, 3),
            "contour_complexity": len(seg.contour_points),
            "fragmentation": seg.fragmentation,
            "garment_bbox_norm": seg.bbox_norm or [0, 0, 0, 0],
            "label_bbox_norm": label_bbox_norm or [0, 0, 0, 0],
        },
        "fabric_analysis": fabric,
        "color_extraction": colors,
        "garment_categorization": gemini.get("garment_categorization", {}),
        "construction_details": construction,
        "brand_information": gemini.get("brand_and_labels", {}),
        "distinctive_features": gemini.get("distinctive_features", {}),
        "technical_specifications": {
            **gemini.get("technical_details", {}),
            "size_estimation": gemini.get("size_estimation", {}),
            "condition": gemini.get("condition_assessment", {}),
        },
        "quality_validation": {
            "overall_confidence": self._calculate_overall_confidence(quality, seg, gemini),
            "ready_for_generation": self._assess_generation_readiness(quality, seg),
            "recommended_improvements": self._suggest_improvements(quality, seg),
        },
        "schema_version": "1.1.0",
        "schema_compliance": True,
    }
    return out

# ---------------------------
# Helpers
# ---------------------------
def _calculate_overall_confidence(
    self, quality: QualityMetrics, seg: SegmentationResult, gemini: Dict[str, Any]
) -> float:
    classification_conf = (
        gemini.get("garment_categorization", {})
        .get("primary_classification", {})
        .get("classification_confidence", 0.5)
    )
    scores = [quality.overall_score, seg.confidence_score, float(classification_conf)]
    return round(float(np.mean(scores)), 3)

def _assess_generation_readiness(self, quality: QualityMetrics, seg: SegmentationResult) -> bool:
    mask_cov = float(np.sum(seg.garment_mask > 0)) / max(1, seg.garment_mask.size)
    boundary_smoothness = 1.0 - float(seg.boundary_irregularity)
    return (
        quality.overall_score > 0.6
        and seg.confidence_score > 0.5
        and boundary_smoothness > 0.4
        and mask_cov > 0.15
    )

def _suggest_improvements(self, quality: QualityMetrics, seg: SegmentationResult) -> List[str]:
    improvements: List[str] = []
    if quality.resolution_score < 1.0:
        improvements.append("Increase image resolution to at least 768×768 px.")
    if quality.lighting_score < self.quality_thresholds["min_lighting"]:
        improvements.append("Improve lighting: even, diffused key; raise exposure toward mid-gray.")
    if quality.focus_score < self.quality_thresholds["min_focus"]:
        improvements.append("Ensure sharp focus across garment (reduce motion/camera shake).")
    if seg.background_complexity == "complex":
        improvements.append("Use a simpler background for better segmentation.")
    if seg.confidence_score < 0.7:
        improvements.append("Center garment fully and avoid occlusions.")
    mask_cov = float(np.sum(seg.garment_mask > 0)) / max(1, seg.garment_mask.size)
    if mask_cov < 0.15:
        improvements.append("Reframe to include entire garment in frame.")
    return improvements or ["Image quality acceptable"]

def _classify_lighting(self, score: float) -> str:
    return "excellent" if score > 0.8 else ("adequate" if score > 0.6 else "poor")

def _classify_focus(self, score: float) -> str:
    return "sharp" if score > 0.8 else ("acceptable" if score > 0.5 else "blurry")

def _get_color_name(self, rgb: List[int]) -> str:
    r, g, b = rgb
    h, s, v = self._rgb_to_hsv_deg(r, g, b)
    if v < 20:
        return "black"
    elif v > 95 and s < 5:
        return "white"
    elif s < 10:
        return "gray"
    elif h < 20 or h > 340:
        return "red"
    elif h < 45:
        return "orange"
    elif h < 70:
        return "yellow"
    elif h < 150:
        return "green"
    elif h < 200:
        return "cyan"
    elif h < 260:
        return "blue"
    elif h < 290:
        return "purple"
    else:
        return "magenta"

def _rgb_to_hsv_deg(self, r: int, g: int, b: int) -> Tuple[float, float, float]:
    rf, gf, bf = r / 255.0, g / 255.0, b / 255.0
    mx = max(rf, gf, bf)
    mn = min(rf, gf, bf)
    diff = mx - mn
    if diff == 0:
        h = 0.0
    elif mx == rf:
        h = (60 * ((gf - bf) / diff) + 360) % 360
    elif mx == gf:
        h = (60 * ((bf - rf) / diff) + 120) % 360
    else:
        h = (60 * ((rf - gf) / diff) + 240) % 360
    s = 0.0 if mx == 0 else (diff / mx) * 100.0
    v = mx * 100.0
    return h, s, v

def _empty_color_data(self) -> Dict[str, Any]:
    return {
        "primary_color": {
            "hex_value": "#000000",
            "lab_values": [0, 0, 0],
            "rgb_values": [0, 0, 0],
            "color_name": "unknown",
        },
        "secondary_colors": [],
        "pattern_analysis": {"pattern_type": "unknown", "pattern_scale": "unknown", "pattern_complexity": 0.0},
        "color_distribution": {"dominant_percentage": 0.0, "color_variety": 0},
    }

def _default_gemini_response(self) -> Dict[str, Any]:
    return {
        "garment_categorization": {
            "primary_classification": {
                "category": "unknown",
                "specific_type": "unknown",
                "subcategory": "unknown",
                "classification_confidence": 0.0,
            },
            "style_attributes": {
                "silhouette": "unknown",
                "neckline": "unknown",
                "sleeves": "unknown",
                "hemline": "unknown",
                "fit_type": "unknown",
            },
        },
        "brand_and_labels": {
            "brand_visible": False,
            "brand_name": "unknown",
            "label_text": "",
            "label_position": "unknown",
            "care_labels_visible": False,
        },
        "distinctive_features": {
            "unique_elements": [],
            "prints_or_graphics": "none",
            "embroidery": False,
            "appliques": False,
            "distressing": False,
        },
        "size_estimation": {"apparent_size": "unknown", "length_category": "regular", "width_category": "regular"},
        "condition_assessment": {"overall_condition": "new", "visible_defects": [], "presentation_quality": 0.8},
        "technical_details": {"seam_visibility": False, "construction_quality": "unknown", "finishing_details": [], "symmetric": True},
    }

def _validate_json_hard(self, payload: Dict[str, Any]) -> None:
    try:
        from jsonschema import Draft202012Validator  # type: ignore
    except Exception:
        # Soft path if jsonschema not installed
        missing = [k for k in SCHEMA["required"] if k not in payload]
        if missing:
            raise ValueError(f"Schema missing required fields: {missing}")
        return

    v = Draft202012Validator(SCHEMA)  # type: ignore
    errs = sorted(v.iter_errors(payload), key=lambda e: list(e.path))  # type: ignore
    if errs:
        msgs = "; ".join([f"{'/'.join(map(str, e.path)) or '<root>'}: {e.message}" for e in errs])  # type: ignore
        raise ValueError("Schema errors: " + msgs)

```

# ---------------------------------------------------------------------------

# Quality Validator

# ---------------------------------------------------------------------------

class QualityValidator:
def **init**(self):
self.metrics_history: List[Dict[str, Any]] = []

```
def validate_json_schema(self, json_data: Dict[str, Any]) -> Tuple[bool, List[str]]:
    required_fields = [
        "processing_metadata",
        "input_validation",
        "segmentation_results",
        "fabric_analysis",
        "color_extraction",
        "garment_categorization",
        "construction_details",
    ]
    errors: List[str] = []
    for f in required_fields:
        if f not in json_data:
            errors.append(f"Missing required field: {f}")

    iv = json_data.get("input_validation", {})
    if not isinstance(iv.get("image_quality_score", 0), (int, float)):
        errors.append("image_quality_score must be numeric")

    sr = json_data.get("segmentation_results", {})
    cs = sr.get("confidence_score", 0)
    if not (isinstance(cs, (int, float)) and 0 <= cs <= 1):
        errors.append("confidence_score must be between 0 and 1")

    return len(errors) == 0, errors

def calculate_consistency_improvement(
    self, current: Dict[str, Any], baseline: Optional[Dict[str, float]] = None
) -> float:
    if baseline is None:
        baseline = {"quality_score": 0.5, "segmentation_confidence": 0.4, "color_accuracy": 0.6, "detail_preservation": 0.5}
    current_scores = [
        current.get("input_validation", {}).get("image_quality_score", 0.0),
        current.get("segmentation_results", {}).get("confidence_score", 0.0),
        0.8,  # placeholder for color accuracy
        0.7,  # placeholder for detail preservation
    ]
    baseline_avg = float(np.mean(list(baseline.values())))
    current_avg = float(np.mean(current_scores))
    if baseline_avg == 0:
        return 0.0
    return round(((current_avg - baseline_avg) / baseline_avg) * 100.0, 2)

def generate_quality_report(self, json_data: Dict[str, Any]) -> Dict[str, Any]:
    valid, errors = self.validate_json_schema(json_data)
    report = {
        "schema_valid": valid,
        "validation_errors": errors,
        "quality_metrics": {
            "preprocessing_quality": json_data.get("input_validation", {}).get("image_quality_score", 0.0),
            "segmentation_confidence": json_data.get("segmentation_results", {}).get("confidence_score", 0.0),
            "overall_confidence": json_data.get("quality_validation", {}).get("overall_confidence", 0.0),
            "generation_ready": json_data.get("quality_validation", {}).get("ready_for_generation", False),
        },
        "consistency_improvement": self.calculate_consistency_improvement(json_data),
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%S"),
    }
    self.metrics_history.append(report)
    return report

```

# ---------------------------------------------------------------------------

# CLI / Batch Runner

# ---------------------------------------------------------------------------

def run_single(pre: AdvancedGarmentPreprocessor, image_path: str, out_dir: Path) -> None:
out = out_dir / Path(image_path).stem
out.mkdir(parents=True, exist_ok=True)
pre.process_garment(image_path, out_dir=out)
[logger.info](http://logger.info/)(f"Saved outputs to {out}")

def main():
ap = argparse.ArgumentParser(description="[Photostudio.io](http://photostudio.io/) — Step 1 Preprocessing")
ap.add_argument("input", help="Path to image file or directory")
ap.add_argument("-o", "--output", default="output", help="Output directory root")
ap.add_argument("--no-gemini", action="store_true", help="Disable Gemini JSON analysis")
ap.add_argument("--gemini-key", default=None, help="Gemini API key (or env GEMINI_API_KEY)")
ap.add_argument("--target", type=int, default=1536, help="Target square size (px)")
args = ap.parse_args()

```
pre = AdvancedGarmentPreprocessor(
    gemini_api_key=args.gemini_key,
    target_size=args.target,
    enable_gemini=not args.no_gemini,
)

in_path = Path(args.input)
out_root = Path(args.output)
out_root.mkdir(parents=True, exist_ok=True)

if in_path.is_dir():
    images: List[Path] = []
    for ext in ("*.jpg", "*.jpeg", "*.png", "*.webp", "*.tif", "*.tiff"):
        images.extend(in_path.rglob(ext))
    if not images:
        logger.error("No images found in directory.")
        return
    for p in images:
        try:
            run_single(pre, str(p), out_root)
        except Exception as e:
            logger.exception(f"Failed {p}: {e}")
else:
    run_single(pre, str(in_path), out_root)

```

if **name** == "**main**":
main()